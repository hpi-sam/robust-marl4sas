import sysfrom scipy import statsfrom tqdm import tqdmimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltplt.rcParams["font.family"] = "Times New Roman"import warningswarnings.filterwarnings("ignore")ROUNDED_ACCURACY = 4def transform_data(data: pd.DataFrame) -> pd.DataFrame:    transformed = data    for index, row in transformed.iterrows():        raw = row['raw']        transformed.loc[index, 'sqt'] = np.sqrt(raw)  # is the same as np.power(raw, (1 / 2))        transformed.loc[index, 'cube'] = np.power(raw, (1 / 3))        np.seterr(divide='ignore')        transformed.loc[index, 'log10'] = np.where(raw > 0, np.log10(raw), 0)        transformed.loc[index, 'ln'] = np.where(raw > 0, np.log(raw), 0)        transformed.loc[index, 'log2'] = np.where(raw > 0, np.log2(raw), 0)    return transformed.round(ROUNDED_ACCURACY)def get_ordering(data: pd.DataFrame) -> pd.DataFrame:    '''    The ordering determines the sequence of the shifting of the <component,failure> groups.    :param data: the dataset    :return: A data frame with the mean values for each <component,failure> group sorted by the mean value, component name.    '''    mean_values_of_the_groups = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].mean().reset_index()    return mean_values_of_the_groups.sort_values(by=[data.columns[2], data.columns[0]], ascending=True)def shift_data(data: pd.DataFrame, spread_multiplication_factor: int = 1, min_std=0.001):    ordering = get_ordering(data)    # set all standard deviation with zero to one    stdev_values = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].std().reset_index().fillna(min_std)    stdev_values.loc[stdev_values[stdev_values.columns[2]] == 0, stdev_values.columns[2]] = min_std    data_new = data.copy()    previous = None    tie_count = 0    for _, values in ordering.iterrows():        if previous is not None:            # standard deviation            std_pre = stdev_values[(stdev_values[data.columns[0]] == previous[0]) & (stdev_values[data.columns[1]] == previous[1])][data.columns[2]].tolist()[0]            std_cur = stdev_values[(stdev_values[data.columns[0]] == values[0]) & (stdev_values[data.columns[1]] == values[1])][data.columns[2]].tolist()[0]            # mean value            mean_pre = previous[2]            mean_cur = values[2]            # check for ties            if mean_cur == mean_pre:                tie_count += std_pre            # shift the data            spread = (std_pre + std_cur) * spread_multiplication_factor + tie_count            data_new.loc[(data_new[data.columns[0]] == values[0]) & (data_new[data.columns[1]] == values[1]), data.columns[2]] += spread        previous = values    return data_new.round(ROUNDED_ACCURACY)def ARol(start, mu, N, theta=0.1, sigma=1):    '''    An auto-regressive model combined with an Ornsteinâ€“Uhlenbeck procedure.    :param start: the starting value of the series -> max of a <componenten, failure>    :param mu: value to end with -> mean of a <componenten, failure>    :param theta: how fast to converge -> fixed to 0.1    :param N: number of series points to create    :return: generated series    '''    series = [start]    for t in range(N):        series.append(series[-1] + theta * (mu - series[-1]) + sigma * np.random.normal(0., 1))    return seriesdef GARCH(mean, N, epsilon=0.1, alpha=0.0001, beta=0.1):    '''    :param mean: the starting value of the series and central point of the time series    :param N: number of series points to create    :param epsilon: a factor    :param alpha: how much the previous series point has an inluence on the new value -> high parameter = high variance    :param beta: how much the noise has in influence    :return: generated series    '''    n1 = 50  # data points to drop    n2 = N + n1  # sum of two numbers    noise = np.random.normal(0., 1, n2)  # the variance of the series, a random sample from a distribution    series = [mean]    for t in range(n2):        variance = np.sqrt(epsilon + alpha * series[t-1]**2 + beta * noise[t-1]**2)        series.append(noise[t] * variance + mean)    return series[n1-1:-1]def create_non_stationary_data(model: str, data: pd.DataFrame, N=100, distinguishable=False):    '''    :param model: choose between AR_ol, GARCH    :param data: data to work on    :param ARCH_theta:    :param N: number of series points to create    :return: a pandas dataframe with non-stationary series for each <component,failure> combination    '''    evaluated_data = data.groupby([data.columns[0], data.columns[1]])[data.columns[2]].agg([max, 'mean']).reset_index()    # empty dataframe for saving the new series non stationary series points    non_stationary_series = pd.DataFrame(columns=[data.columns[0], data.columns[1], data.columns[2]])    # iterate through all <component_failure> groups    for index, group in tqdm(evaluated_data.iterrows(), total=evaluated_data.shape[0]):        # create the series points using a model        series = []        if model == 'ARol':            series = ARol(group['max'], group['mean'], N)        elif model == 'GARCH':            series = GARCH(group['mean'], N)        else:            print('Model is not provided.')            sys.exit(0)        # saving series points in dataframe        for s in series:            new_row = pd.DataFrame({data.columns[0]: group[0], data.columns[1]: group[1], data.columns[2]: s}, index=[0])            non_stationary_series = non_stationary_series.append(new_row, ignore_index=True)        # plot series        plt.plot(series)    dist = '_dist' if distinguishable else ''    plt.xlabel('Time')    plt.ylabel('Time Series Value')    plt.savefig('data_analysis/04_plots/nonstationary_' + data.columns[2] + '_' + model + '_' + data.columns[2] + dist + '.pdf')    plt.show()    return non_stationary_series.round(ROUNDED_ACCURACY)def execute_ttest(shifted_data: pd.DataFrame) -> pd.DataFrame:    ttest_results = pd.DataFrame(columns=['component_1', 'failure_1', 'component_2', 'failure_2', 'statistic', 'pvalue'])    ordering = get_ordering(shifted_data)    data_grouped = shifted_data.groupby([shifted_data.columns[0], shifted_data.columns[1]])[shifted_data.columns[2]].apply(list).reset_index()    # evaluation starts here    previous = None    for index, name in ordering.iterrows():        if previous is not None:            # get the values for the previous and current <component, failure> group            values_pre = data_grouped.loc[(data_grouped[data_grouped.columns[0]] == previous[0]) & (data_grouped[data_grouped.columns[1]] == previous[1])][data_grouped.columns[2]].tolist()[0]            values_cur = data_grouped.loc[(data_grouped[data_grouped.columns[0]] == name[0]) & (data_grouped[data_grouped.columns[1]] == name[1])][data_grouped.columns[2]].tolist()[0]            # execute ttest            result = stats.ttest_ind(values_pre, values_cur)            new_row = {'component_1': previous[0], 'failure_1': previous[1],                        'component_2': name[0], 'failure_2': name[1],                        'statistic': result[0], 'pvalue': result[1] if result[1] != np.NaN else 1}            ttest_results = ttest_results.append(new_row, ignore_index=True)        previous = name    return ttest_resultsdef get_distinguishable_groups(ttest_results: pd.DataFrame, significance_level: float = 0.05) -> [()]:    '''    Returns a list of tuples of <component,failure> groups which are considered to be distinguishable.    :param ttest_results: A dataframe with the ttest results for a gorup pair.    :param significance level: by default 0.05    :return: A list of tuples with distinguishable group pairs.    '''    distinguisable_pairs = ttest_results.loc[(ttest_results['pvalue'] < significance_level), ['component_1', 'failure_1', 'component_2', 'failure_2']]    first_list = list(zip(distinguisable_pairs.component_1, distinguisable_pairs.failure_1))    second_list = list(zip(distinguisable_pairs.component_2, distinguisable_pairs.failure_2))    return list(set(first_list + second_list))def filter_dataset(data: pd.DataFrame, component_failure_list: list) -> pd.DataFrame:    '''    Returns a dataframe with only these <component,failure> groups which are in the component_failure_list.    :param data: dataframe to be filtered    :param component_failure_list: a list of tuples, which indicates all <component,failure> groups to be maintained in the dataset.    :return: a filtered dataset    '''    filtered_data = pd.DataFrame(columns=data.columns)    for cf in component_failure_list:        selection = data[((data[data.columns[0]] == cf[0]) & (data[data.columns[1]] == cf[1]))]        filtered_data = pd.concat([filtered_data, selection], sort=False, ignore_index=True)    return filtered_data